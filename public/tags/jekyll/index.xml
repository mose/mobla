<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jekyll on Moselog</title>
    <link>http://localhost:1313/tags/jekyll/</link>
    <description>Recent content in Jekyll on Moselog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Feb 2015 12:10:31 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/jekyll/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Greenruby 105</title>
      <link>http://localhost:1313/2015/02/08/greenruby-105</link>
      <pubDate>Sun, 08 Feb 2015 12:10:31 +0800</pubDate>
      
      <guid>http://localhost:1313/2015/02/08/greenruby-105</guid>
      <description>

&lt;h2 id=&#34;damn-omnivore:f97d1953e92cfdfb05d8ef20de1a8b30&#34;&gt;Damn omnivore&lt;/h2&gt;

&lt;p&gt;Last week Green Ruby was sent with a delay. For some obscure reason my
publication was blocked by &lt;a href=&#34;http://mailchimp.com/omnivore/&#34;&gt;Omnivore&lt;/a&gt; the automated fraud detection system
from Mailchimp. My account was instantly blocked and it was pending a human
review. Of course the human review made it clear that the publication was
clean but it took 2 days and I still don&amp;rsquo;t know why this omnivore beast went
mad at me.&lt;/p&gt;

&lt;p&gt;They declare it&amp;rsquo;s getting smarter everyday, but really, we are still safe. The
reign of the intelligent computer over the human species is not close yet.&lt;/p&gt;

&lt;h2 id=&#34;make-it-static:f97d1953e92cfdfb05d8ef20de1a8b30&#34;&gt;Make it static&lt;/h2&gt;

&lt;p&gt;Dynamic websites are great. But that&amp;rsquo;s a long time I wonder about the
trade-off. If you update your blog every day, and have 500 visits, your
dynamic setup is useful when you edit, and it&amp;rsquo;s a cost for each visitor. There
are so many web applications that could be more clever about it. Especially
that now the computation is going more and more client side.&lt;/p&gt;

&lt;p&gt;Static pages have a really unbeatable response time, their security is really
reliable, they are low dependencies and easy to deploy with a rsync.&lt;/p&gt;

&lt;p&gt;When I get to think about making a new website, I always ask myself if it&amp;rsquo;s an
application or a website. For sure some kind of applications are computation
intensive. Making it all dynamic could make sense. But frankly, if it&amp;rsquo;s a
website, it can have some fancy dynamic features without a huge dynamic setup.&lt;/p&gt;

&lt;p&gt;I have seen so many websites made with php, mysql, 5 tables and 20 entries in
each. Such site should have pre-generated content, data available as static
json files, as a bit of js to make the magic happen. If there is a lot of
data, fine, an API server makes sense.&lt;/p&gt;

&lt;p&gt;So next time you gotta prepare a website, ask yourself how easy it could be to
generate it and use tools like &lt;a href=&#34;http://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt; or &lt;a href=&#34;https://middlemanapp.com/&#34;&gt;Middleman&lt;/a&gt;. Or you can even
handle things with custom rake tasks (&lt;a href=&#34;https://github.com/greenruby/grn-static/blob/master/lib/builder.rb#L87&#34;&gt;like I do for Green Ruby website&lt;/a&gt;),
that&amp;rsquo;s not that hard. It&amp;rsquo;s a matter of cyberspace ecology.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>