<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Devops on Moselog</title>
    <link>http://blog.mose.com/tags/devops/</link>
    <description>Recent content in Devops on Moselog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 May 2015 12:10:31 +0800</lastBuildDate>
    <atom:link href="http://blog.mose.com/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>linux trick: too many logs</title>
      <link>http://blog.mose.com/2015/05/31/linux-trick-too-many-logs</link>
      <pubDate>Sun, 31 May 2015 12:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2015/05/31/linux-trick-too-many-logs</guid>
      <description>&lt;p&gt;Recently I found my self again in that situation on a linux server. The
partition where logs are stored went 100%. In such case, It&amp;rsquo;s clever top purge
old useless logfiles. Typical move for me would be to run logrotate manually
with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;logrotate -f /etc/logrotate.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But I had a case where that was not enough. A developer forgot to remove a
debugging output and the logs were just gathering way too much information,
more than what I could free with some janitoring.&lt;/p&gt;

&lt;p&gt;To avoid losing logs, we can move the logfile where there is space and replace
the file with a symbolic link. That&amp;rsquo;s good enough for until the partition gets
resized of the logs get cleaned. But when it&amp;rsquo;s done on a live logfile, the
running process that writes into it still has the same file descriptor. The
process has to be relaunched so the new fd can be taken in account, on the new
partition, as instructed by the symbolic link.&lt;/p&gt;

&lt;p&gt;So a colleague pointed out that could be done without restart by using gdb.
It&amp;rsquo;s a pretty neat trick (if you have gdb installed on your production server,
which may not always be the case, and for good reasons). Anyways I had it at
hand, and here is the sequence:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch /path/to/new/logfile

gdb -p pid

(gdb) call dup(1)
$1 = 3
(gdb) call close(1)
$2 = 0
(gdb) call open(&amp;quot;/path/to/new/logfile&amp;quot;, 2)
$3 = 1
(gdb) call close($1)
$4 = 0
(gdb)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gave me the taste of digging up a little bit more on how gdb can interact
with live processes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Greenruby 101</title>
      <link>http://blog.mose.com/2015/01/11/greenruby-101</link>
      <pubDate>Sun, 11 Jan 2015 12:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2015/01/11/greenruby-101</guid>
      <description>

&lt;h2 id=&#34;1000-subscribers:dfd863a69df72c13bbe5f0b5d11c54ce&#34;&gt;1000 subscribers&lt;/h2&gt;

&lt;p&gt;After the 100th Greenruby last week, we get the 1000th subscriber to the email
newsletter this week. Welcome George :) So for the occasion I refreshed the
&lt;a href=&#34;http://mose.cartodb.com/viz/6a4d431e-d6b7-11e3-8757-0e10bcd91c2b/embed_map&#34;&gt;subscribers map&lt;/a&gt; on cartodb. About half of the subscribers are in the US,
but there is a total of 73 countries represented, which is pretty neat. But
this is based on the ip used for subscribing, so it&amp;rsquo;s not totally accurate.&lt;/p&gt;

&lt;h2 id=&#34;fighting-logs-pollution:dfd863a69df72c13bbe5f0b5d11c54ce&#34;&gt;Fighting logs pollution&lt;/h2&gt;

&lt;p&gt;Some time ago, on edition 82, I posted a link to my webalizer stats for the
greenruby website. Fatal mistake, I had referrers stats enabled and I got
somehow listed somewhere. The consequence was a lot of fake traffic with only
purpose to get links listed in my referrers section. After removing this
section from webalizer config, and moving the url, the fake traffic was still
there. Like a blind wave. This was pretty annoying.&lt;/p&gt;

&lt;p&gt;So I began to take some drastic measures. Because I didn&amp;rsquo;t have referrers
anymore in my stats, I went to my apache logs (yes it&amp;rsquo;s an old server, still
running apache2) and fire up a:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tail -5000 greenruby.org-access.log | cut -d&#39; &#39; -f11 | sort | uniq -c | sort -n
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This give me a nice view of the recent referrers. Fake ones are easy to
notice. And that the fake traffic was only coming from a handful of ips, so I
just made a few:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;iptables -A INPUT -s &amp;lt;ip&amp;gt; -j DROP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result was radically efficient. Certainly I had less volume but it&amp;rsquo;s now
much cleaner. But it makes it clear that having clean volume stats for traffic
on your website is not that easy. There are a bunch of fake traffic sources
that you may not suspect.&lt;/p&gt;

&lt;h2 id=&#34;new-server-soon:dfd863a69df72c13bbe5f0b5d11c54ce&#34;&gt;New server soon&lt;/h2&gt;

&lt;p&gt;I got a new job 2 month ago at &lt;a href=&#34;http://gandi.net&#34;&gt;gandi&lt;/a&gt;, and there is some nice &lt;a href=&#34;https://www.gandi.net/hosting/iaas&#34;&gt;VPS
hosting&lt;/a&gt; there. I can have a machine just for Green Ruby there. It can be
neat and open some options. Green Ruby website is still very static. Various
attempts to improve it with a search engine didn&amp;rsquo;t end up to something
concluding yet. Maybe this time it will.&lt;/p&gt;

&lt;p&gt;That also may be the occasion to build up a distribution system and move out
of mailchimp. I looked around and all I found was php based. So I will
probably just write my own ruby scripts, the &lt;a href=&#34;https://github.com/mikel/mail&#34;&gt;mail gem&lt;/a&gt; looks great and the
rest is a question of configuring a postfix with correct &lt;a href=&#34;http://www.linuxlasse.net/linux/howtos/Postfix_with_DKIM_%28OpenDKIM%29_and_SPF&#34;&gt;SPF and DKIM
setup&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;je-suis-charlie:dfd863a69df72c13bbe5f0b5d11c54ce&#34;&gt;Je suis Charlie&lt;/h2&gt;

&lt;p&gt;Well, this tragedy that happened this Thursday in France impacts a lot of
people. Somehow, it perhaps impacts me more than the average. When I was
younger I went to art school to become a cartoonist, and finally I changed my
mind. But I knew the work of the victims of this slaughter. They were icons in
the French cartoon world.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s really sad, first because this is murder, second because the irrational
impact it will have on society. This is crazy how a handful of brain-dead
punks can bend history. Now France is going to become even more paranoid. I
already was so uncomfortable when they began to send military with assault
rifles wander in the train stations in Paris. It&amp;rsquo;s not going to get better.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Digitalocean CLI</title>
      <link>http://blog.mose.com/2014/08/07/digitalocean-cli</link>
      <pubDate>Thu, 07 Aug 2014 09:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2014/08/07/digitalocean-cli</guid>
      <description>&lt;p&gt;We host some dev boxes on Digitalocean, and I tried varioous CLI because I like to stay in the console. Recently I was pretty happy to find Tugboat, as it saves default droplet size, region and image, and convers the whole API. Its fuzzy droplet name matching also can find great usage.&lt;/p&gt;

&lt;p&gt;The only problem it has, is its counter-intuitive damn name (I forgot how to launch it 2 hours after using it the first time haha).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pearkes/tugboat&#34;&gt;https://github.com/pearkes/tugboat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;img src=&#34;http://res.cloudinary.com/mosepix/image/upload/devtips/2014-08-07-tugboat.png&#34; alt=&#34;tugboat&#34; class=&#34;pure-img&#34; /&gt;


</description>
    </item>
    
    <item>
      <title>Knife remote command</title>
      <link>http://blog.mose.com/2014/06/26/knife-remote-command</link>
      <pubDate>Thu, 26 Jun 2014 09:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2014/06/26/knife-remote-command</guid>
      <description>&lt;p&gt;Recently we switched from chef-solo to a chef-server setup on our infrastructure, a good occasion to refactor our recipes to better practices. I spent some time figuring out how to replace the &lt;code&gt;fabric&lt;/code&gt; scripts I had for remote execution of actions on various servers, by using a knife plugin. That way I can just use knife abilities and don&amp;rsquo;t need fabric anymore.&lt;/p&gt;

&lt;p&gt;So I created a new file in &lt;code&gt;.chef/plugins/knife/&lt;/code&gt; named &lt;code&gt;apt.rb&lt;/code&gt; for a test:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;chef/knife&#39;

module KnifeOpenvpn
  class Apt &amp;lt; Chef::Knife

    banner &amp;quot;knife apt &amp;lt;update|upgrade|simulate&amp;gt; &amp;lt;nodename&amp;gt;&amp;quot;

    deps do
      require &#39;chef/knife/ssh&#39;
      require &#39;chef/node&#39;
      Chef::Knife::Ssh.load_deps
    end

    def run
      if name_args.size == 2
        command_arg = name_args.shift
        server = name_args.shift
      else
        ui.fatal &amp;quot;Syntax: knife apt &amp;lt;update|upgrade|simulate&amp;gt; &amp;lt;nodename&amp;gt;&amp;quot;
        ui.fatal &amp;quot;Where &amp;lt;nodename&amp;gt; is a node name.&amp;quot;
        exit 1
      end
      command = case command_arg
                when &#39;update&#39;
                  &#39;update&#39;
                when &#39;upgrade&#39;
                  &#39;-y upgrade&#39;
                when &#39;simulate&#39;
                  &#39;-y -s upgrade&#39;
                end
      knife_ssh(Chef::Node.load(server).ipaddress, &amp;quot;sudo apt-get #{command}&amp;quot;).run
    end

    def knife_ssh(server, command)
      ssh = Chef::Knife::Ssh.new
      ssh.name_args = [ server, command ]
      ssh.config[:ssh_user] = Chef::Config[:knife][:ssh_user]
      ssh.config[:ssh_port] = Chef::Config[:knife][:ssh_port]
      ssh.config[:identity_file] = Chef::Config[:knife][:identity_file]
      ssh.config[:ssh_gateway] = Chef::Config[:knife][:ssh_gateway]
      ssh.config[:manual] = true
      ssh.config[:host_key_verify] = Chef::Config[:knife][:host_key_verify]
      ssh.config[:on_error] = :raise
      ssh
    end

  end

end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I just run a &lt;code&gt;knife apt simulate my-server&lt;/code&gt; to execute a &lt;code&gt;apt-get -s -y upgrade&lt;/code&gt; on the &lt;code&gt;my-server&lt;/code&gt; client node. Pretty useful. But I guess that&amp;rsquo;s only a beginning, I should extend it to run on various nodes at the same time and maybe inside threads or something like that, to match the &lt;code&gt;fabric&lt;/code&gt; power.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>S3 backups</title>
      <link>http://blog.mose.com/2014/06/12/s3-backups</link>
      <pubDate>Thu, 12 Jun 2014 09:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2014/06/12/s3-backups</guid>
      <description>&lt;p&gt;We use S3 to backup various kind of files on MB. We use the very convenient backup gem for that (we still use 3.9.0).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://meskyanichi.github.io/backup/v4/&#34;&gt;http://meskyanichi.github.io/backup/v4/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But at some point it appeared that backing up our audio recording was hammering disk IO on our server, because the syncer is calculating md5 footprint for each file each time a backup happens. When you get thousands of big files that is pretty expensive process (in our case 20k files and 50G total).&lt;/p&gt;

&lt;p&gt;So I added a small trick there:&lt;/p&gt;

&lt;p&gt;in &lt;code&gt;Backup/models/backup_audio.rb&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;module Backup::Syncer::Cloud
  class Base &amp;lt; Syncer::Base
    def process_orphans
      if @orphans.is_a?(Queue)
        @orphans = @orphans.size.times.map { @orphans.shift }
      end
      &amp;quot;Older Files: #{ @orphans.count }&amp;quot;
    end
  end
end

Backup::Model.new(:backup_audio, &#39;Audio files Backup to S3&#39;) do

  before do
    system(&amp;quot;/Backup/latest_audio.sh&amp;quot;)
  end

  after do
    FileUtils.rm_rf(&amp;quot;/tmp/streams&amp;quot;)
  end

  ##
  # Amazon Simple Storage Service [Syncer]
  #
  sync_with Cloud::S3 do |s3|
    s3.access_key_id     = &amp;quot;xxx&amp;quot;
    s3.secret_access_key = &amp;quot;xxx&amp;quot;
    s3.bucket            = &amp;quot;bucket_backup&amp;quot;
    s3.region            = &amp;quot;us-east-1&amp;quot;
    s3.path              = &amp;quot;/mb_audio_backup&amp;quot;
    s3.mirror            = false
    s3.thread_count      = 50

    s3.directories do |directory|
      directory.add &amp;quot;/tmp/streams&amp;quot;
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and in &lt;code&gt;Backup/latest_audio.sh&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
# isolate files changed in the last 3 days

TMPDIR=/tmp/streams

mkdir $TMPDIR
for i in `find /storage/audio/ -type f -cmin -4320`; do
  ln -s $i $TMPDIR
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It creates a fake backup dir with links to the files that actually changed in the last 3 days and patches the syncer to avoid flooding the logs with orphan files. When sometimes S3 upload fails on one file (and it happens from time to time for &amp;lsquo;amazonian&amp;rsquo; reason) it will be caught on the next daily backup.&lt;/p&gt;

&lt;p&gt;The result was pretty obvious on our disk usage with our daily backups:&lt;/p&gt;

&lt;img src=&#34;http://res.cloudinary.com/mosepix/image/upload/devtips/2014-06-12-s3backup.png&#34; alt=&#34;graph&#34; class=&#34;pure-img&#34; /&gt;



&lt;p&gt;in the logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2014/06/10 07:00:25][info] Summary:
[2014/06/10 07:00:25][info]   Transferred Files: 5
[2014/06/10 07:00:25][info]   Older Files: 22371
[2014/06/10 07:00:25][info]   Unchanged Files: 16
[2014/06/10 07:00:25][info] Syncer::Cloud::S3 Finished!
[2014/06/10 07:00:25][info] Backup for &#39;Audio files Backup to S3 (backup_audio)&#39; Completed Successfully in 00:00:22
[2014/06/10 07:00:25][info] After Hook Starting...
[2014/06/10 07:00:25][info] After Hook Finished.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Install BigBlueButton in a VM on Ubuntu 1204</title>
      <link>http://blog.mose.com/2013/03/22/install-bigbluebutton-in-a-vm-on-ubuntu-1204</link>
      <pubDate>Fri, 22 Mar 2013 09:10:31 +0800</pubDate>
      
      <guid>http://blog.mose.com/2013/03/22/install-bigbluebutton-in-a-vm-on-ubuntu-1204</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://bigbluebutton.org/&#34;&gt;BigBlueButton&lt;/a&gt; is an amazing piece of free software, designed for virtual classrooms but can also be used for webinars, conferences, technical support and other uses. It gathers in the same place a video-conference system, with a shared PDF on which the presenter can doodle, an SIP bridge that make people can join using a phone, and a deskshare java applet for showing others pieces of your screen. Pretty neat. It’s around since 2007 and I had occasion to play with it with the Tikiwiki community.&lt;/p&gt;

&lt;p&gt;And now I need to install one at &lt;a href=&#34;http://codegreenit.com/&#34;&gt;Codegreen&lt;/a&gt; for our customer support and maybe some other uses. Great. I go for the last version 0.80 and it only works on ubuntu 10.04 (there are 25 different free software projects embedded in the beast, quite hard to maintain all on many versions, so they stick to the old one for now it seems).&lt;/p&gt;

&lt;p&gt;So I get a new server just for that, and I try the install of the old 10.04 image. But it fails miserably for some new SATA drivers reasons, despite the more recent 12.04 works fine, the 10.04 just can’t see the hard drive. Bummer. After some fight then I need to use the alternative install method, using the &lt;a href=&#34;http://code.google.com/p/bigbluebutton/wiki/BigBlueButtonVM&#34;&gt;VM image&lt;/a&gt; that BBB provides for VMWare.&lt;/p&gt;

&lt;h2 id=&#34;install-the-bbb-vm:84bb7b31097e7dff868fbef479daafb5&#34;&gt;Install the BBB VM&lt;/h2&gt;

&lt;p&gt;I don’t really like VMWare but I go get the free player and install it. The VMWare server that I wanted is discontinued they now only offer the VMWare player which requires a GUI install. Very lame, but ok. I get the VMWare Player image from their site and I install it without problem (the .bundle that I download needs to be executed after a chmod +x). Then I get the BBB VM 0.80 and launch it in the VMWare Player and tadaaa, it just works out of the box, magic.&lt;/p&gt;

&lt;p&gt;Note that I installed it under my own account, no need for root with that thing.&lt;/p&gt;

&lt;h2 id=&#34;setup-the-startup-launch:84bb7b31097e7dff868fbef479daafb5&#34;&gt;Setup the startup launch&lt;/h2&gt;

&lt;p&gt;Because the Player is a GUI application, I looked around and found the way to launch it from commandline using the VIX (that is downloadable on the same page as the player and installable the same way). Then I can use vmrun to control the player, feels much better. I add in the /etc/rc.local of the host:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo -u mose /usr/bin/vmrun -T player start /home/mose/vmware/bigbluebutton-vm/bigbluebutton-vm.vmx nogui
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-the-routing:84bb7b31097e7dff868fbef479daafb5&#34;&gt;Handling the routing&lt;/h2&gt;

&lt;p&gt;Here is the tricky part. The Player has 3 ways to handle th routing of the VM (which is very poor compared to the options provided by VirtualBox). and the bridge method was not working for me, because I have a static IP for the server on one card, a dhcp address on internal network on a second card, and the bridge option in VMWare don’t let you specify which interface is going to be bridged.&lt;/p&gt;

&lt;p&gt;BBB says that you should use bridge, so the VM has its own IP and is like a first class member of your network, but it seemed that the NAT option was possible as far as you redirect some ports. So I’m going that way.&lt;/p&gt;

&lt;p&gt;So I setup the NAT option for the VM, in the VM I fix the ip to 192.168.96.6 (that on the host I aliased to vm-bbb in /etc/hosts), and I add the subdomain I plan to use in the /etc/hosts of the VM (that way the VM bypass the DNS search and assumes that his own natted IP is matching the subdomain reachable from outside). Note that this etc-hosts trick will spit some errors at next step that needs to be run on the VM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bbb-conf --setip bbb.mydomain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I need to redirect some ports, like the 22 for ssh access, the 80 for access to the main application, and some others that I have no clue what they are useful for (but I got it from some blog posts). So I install rinetd and add to /etc/rinetd.conf:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.0.0.0 2222 vm-bbb 22
0.0.0.0 80   vm-bbb 80
0.0.0.0 1935 vm-bbb 1935
0.0.0.0 9123 vm-bbb 9123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and all is good. It seems to work fine, and I probably will have to test it more, but besides the sysadmin setup, the install of BBB was pretty straightforward. Great job !&lt;/p&gt;

&lt;h2 id=&#34;links:84bb7b31097e7dff868fbef479daafb5&#34;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Get VMWare player and VIX : &lt;a href=&#34;https://my.vmware.com/web/vmware/free#desktop_end_user_computing/vmware_player/5_0&#34;&gt;https://my.vmware.com/web/vmware/free#desktop_end_user_computing/vmware_player/5_0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BBB website : &lt;a href=&#34;http://bigbluebutton.org&#34;&gt;http://bigbluebutton.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BBB VM instructions : &lt;a href=&#34;http://code.google.com/p/bigbluebutton/wiki/BigBlueButtonVM&#34;&gt;http://code.google.com/p/bigbluebutton/wiki/BigBlueButtonVM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BBB VM Download : &lt;a href=&#34;http://sourceforge.net/projects/bigbluebutton/files/&#34;&gt;http://sourceforge.net/projects/bigbluebutton/files/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>